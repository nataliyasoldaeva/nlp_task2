{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lyricsgenius in /Users/nataliya/anaconda3/lib/python3.7/site-packages (1.8.2)\r\n",
      "Requirement already satisfied: requests>=2.20.0 in /Users/nataliya/anaconda3/lib/python3.7/site-packages (from lyricsgenius) (2.22.0)\r\n",
      "Requirement already satisfied: beautifulsoup4==4.6.0 in /Users/nataliya/anaconda3/lib/python3.7/site-packages (from lyricsgenius) (4.6.0)\r\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/nataliya/anaconda3/lib/python3.7/site-packages (from requests>=2.20.0->lyricsgenius) (3.0.4)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/nataliya/anaconda3/lib/python3.7/site-packages (from requests>=2.20.0->lyricsgenius) (1.24.2)\r\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/nataliya/anaconda3/lib/python3.7/site-packages (from requests>=2.20.0->lyricsgenius) (2.8)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/nataliya/anaconda3/lib/python3.7/site-packages (from requests>=2.20.0->lyricsgenius) (2019.11.28)\r\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install lyricsgenius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nataliya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/nataliya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import lyricsgenius\n",
    "import json\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.lm import MLE\n",
    "from nltk.util import pad_sequence\n",
    "from nltk.util import bigrams\n",
    "from nltk.util import ngrams\n",
    "from nltk.util import everygrams\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk.lm.preprocessing import flatten\n",
    "from nltk.tokenize import ToktokTokenizer\n",
    "from nltk import word_tokenize, sent_tokenize \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')\n",
    "from nltk.lm import KneserNeyInterpolated as KNI\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokenize = lambda x: re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', x)\n",
    "toktok = ToktokTokenizer()\n",
    "word_tokenize = word_tokenize = toktok.tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparing the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"\"Город под подошвой (City Under the Sole)\"\" by Oxxxymiron...\n",
      "Done.\n",
      "Searching for \"«Где нас нет» (”On the Other Side”)\" by Oxxxymiron...\n",
      "Done.\n",
      "Searching for \"До зимы (Before Winter)\" by Oxxxymiron...\n",
      "Done.\n",
      "Searching for \"Неваляшка (Tumbler Toy)\" by Oxxxymiron...\n",
      "Done.\n",
      "Searching for \"Переплетено (Interlaced)\" by Oxxxymiron...\n",
      "Done.\n",
      "Searching for \"В книге всё было по-другому (4 раунд 17ib) (The Book Had It Different)\" by Oxxxymiron...\n",
      "Done.\n",
      "Searching for \"Тентакли (Tentacles)\" by Oxxxymiron...\n",
      "Done.\n",
      "Searching for \"Восточный Мордор (East Mordor)\" by Oxxxymiron...\n",
      "Done.\n",
      "Searching for \"Песенка Гремлина (Gremlin’s Song)\" by Oxxxymiron...\n",
      "Done.\n",
      "Searching for \"Признаки Жизни (Signs of Life)\" by Oxxxymiron...\n",
      "Done.\n",
      "Searching for \"Детектор лжи (Lie Detector)\" by Oxxxymiron...\n",
      "Done.\n",
      "Searching for \"Пролив Дрейка (Drake Passage)\" by Oxxxymiron...\n",
      "Done.\n",
      "Searching for \"Башня из слоновой кости (Ivory Tower)\" by Oxxxymiron...\n",
      "Done.\n",
      "Searching for \"Биполярочка (Bipolarochka)\" by Oxxxymiron...\n",
      "Done.\n",
      "Searching for \"Девочка Пиздец (Fucked Up Girl)\" by Oxxxymiron...\n",
      "Done.\n",
      "Searching for \"Последний звонок (Last Call)\" by Oxxxymiron...\n",
      "Done.\n",
      "Searching for \"Всего лишь писатель (Just a Writer)\" by Oxxxymiron...\n",
      "Done.\n",
      "Searching for \"Привет со дна  (Hello from the Bottom)\" by Oxxxymiron...\n",
      "Done.\n",
      "Searching for \"Не от мира сего (Not of This World)\" by Oxxxymiron...\n",
      "Done.\n",
      "Searching for \"Больше Бена (Bigga Than Ben)\" by Oxxxymiron...\n",
      "Done.\n",
      "Searching for \"Хитиновый покров (Chitin Shell)\" by Oxxxymiron...\n",
      "Done.\n",
      "Searching for \"«Полигон» (“Butts”)\" by Oxxxymiron...\n",
      "Done.\n",
      "Searching for \"Спонтанное самовозгорание (Spontaneous Combustion)\" by Oxxxymiron...\n",
      "Done.\n",
      "Searching for \"Кем ты стал? (What Had You Become?)\" by Oxxxymiron...\n",
      "Done.\n",
      "Searching for \"Russky Cockney\" by Oxxxymiron...\n",
      "Done.\n",
      "Searching for \"Накануне (On the Eve)\" by Oxxxymiron...\n",
      "Done.\n",
      "Searching for \"Darkside\" by Oxxxymiron...\n",
      "Done.\n",
      "Searching for \"Жук в муравейнике (Beetle in an anthill)\" by Oxxxymiron...\n",
      "Done.\n",
      "Searching for \"Дело нескольких минут (3 раунд 17ib) (A Matter of Minutes)\" by Oxxxymiron...\n",
      "Done.\n",
      "Searching for \"Лондон против всех (London Vs. Everyone)\" by Oxxxymiron...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "genius = lyricsgenius.Genius(\"Hm2O62KAtRDaiE9AcUkCmQKivJeVn-7-x1Mj7oXDy0wOzvyAn8L4Jx3XmPjlKOmo\")\n",
    "genius.remove_section_headers = True\n",
    "train_lyrics = ['\"Город под подошвой (City Under the Sole)\"', \"«Где нас нет» (”On the Other Side”)\", \"До зимы (Before Winter)\", \"Неваляшка (Tumbler Toy)\", \"Переплетено (Interlaced)\", \"В книге всё было по-другому (4 раунд 17ib) (The Book Had It Different)\", \"Тентакли (Tentacles)\", \"Восточный Мордор (East Mordor)\", \"Песенка Гремлина (Gremlin’s Song)\", \"Признаки Жизни (Signs of Life)\", \"Детектор лжи (Lie Detector)\", \"Пролив Дрейка (Drake Passage)\", \"Башня из слоновой кости (Ivory Tower)\", \"Биполярочка (Bipolarochka)\", \"Девочка Пиздец (Fucked Up Girl)\", \"Последний звонок (Last Call)\", \"Всего лишь писатель (Just a Writer)\", \"Привет со дна  (Hello from the Bottom)\", \"Не от мира сего (Not of This World)\", \"Больше Бена (Bigga Than Ben)\", \"Хитиновый покров (Chitin Shell)\", \"«Полигон» (“Butts”)\", \"Спонтанное самовозгорание (Spontaneous Combustion)\", \"Кем ты стал? (What Had You Become?)\", \"Russky Cockney\", \"Накануне (On the Eve)\", \"Darkside\", \"Жук в муравейнике (Beetle in an anthill)\", \"Дело нескольких минут (3 раунд 17ib) (A Matter of Minutes)\", \"Лондон против всех (London Vs. Everyone)\"]\n",
    "text = \"\"\n",
    "for index in train_lyrics:\n",
    "    song = genius.search_song(index, 'Oxxxymiron')\n",
    "    text += song.lyrics + \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting into phrases\n",
    "tr_text = re.split(r'\\n', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing punctuation, putting text into lowercase, deleting empty strings\n",
    "import string\n",
    "tr_text = [re.sub(r'[^A-Za-zА-Яа-я0-9ё ]+', '', x) for x in tr_text]\n",
    "\n",
    "tr_text = [x.lower() for x in tr_text]\n",
    "\n",
    "tr_text = [x for x in tr_text if x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting text into words\n",
    "words = []\n",
    "for i in range(len(tr_text)):\n",
    "    t = []\n",
    "    for j in tr_text[i].split():\n",
    "        t.append(j)\n",
    "    words.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying tokenizer to our lyrics split into words\n",
    "list1 = [' '.join(sent) for sent in words]\n",
    "tokens = [list(map(str.lower, word_tokenize(sent))) for sent in list1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['раз', 'на', 'раз', 'баш', 'на', 'баш', 'чё', 'зассал', 'не', 'пацан']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking tokenizing\n",
    "tokens[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2-gram model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, padded_sents = padded_everygram_pipeline(2, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "detokenize = TreebankWordDetokenizer().detokenize\n",
    "\n",
    "#defining a function\n",
    "def generate_sent(model, num_words, random_seed=42):\n",
    "    content = []\n",
    "    for token in model.generate(num_words, random_seed=random_seed):\n",
    "        if token == '<s>':\n",
    "            continue\n",
    "        if token == '</s>':\n",
    "            break\n",
    "        content.append(token)\n",
    "    return detokenize(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting the model\n",
    "model = MLE(2)\n",
    "model.fit(train_data, padded_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5797"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking the length of our vocabulary\n",
    "len(model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ста икон\n",
      "ты с каждым вторым перезнакомься\n",
      "что я был vip до сих пор mc руки к тебе в метро глуховского\n",
      "my whole life on the best is me\n",
      "me\n",
      "на соседей идут следом\n",
      "рунами да правосудие щедрое хер как мощи в кресло вицемэра метит вам признаки жизни в погреб отпираю ржавый медный замок\n",
      "вброд или пинап\n",
      "hacksaw ragh\n",
      "здесь достигли смены парадигмы\n",
      "майкчеку кровью харкал\n",
      "за рупор это круговорот природы червяков доест орёл\n",
      "и против всех\n",
      "ак завтра перевернёт всё здесь видишь glenfiddich\n",
      "школяр в пух перемолот\n",
      "всё переплетено и мне мученье доставили но пятый годкак я за два пути\n",
      "как ты хочешь теперь у таких прицел на бал фитиль был всем как игра в чём не яд\n",
      "подземка не говорят стать толерантным надо жить если за совет братан\n",
      "но ты не надо детский сад дружный класс дважды два метра но времени в каземате прозябает только будущее и не\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    if generate_sent(model,20,random_seed=i) == '':\n",
    "        continue\n",
    "    else:\n",
    "        print(generate_sent(model,20,random_seed=i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3-gram model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, padded_sents = padded_everygram_pipeline(3, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting the model\n",
    "model = MLE(3)\n",
    "model.fit(train_data, padded_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "своим удивительно просто\n",
      "чел ты вспомни мы с ней белиберда\n",
      "был внутри ты забытый на мели\n",
      "как выстрел в лицо\n",
      "коров отвык\n",
      "помереть еретиком\n",
      "плюс и дева это не побег\n",
      "эх день простоять мог ли я себе нередко врал\n",
      "впереди водоворот\n",
      "и мастурбировал\n",
      "видел диапозитив\n",
      "всё предельно серьёзно\n",
      "избавляя избирателя от самотерзаний\n",
      "что\n",
      "dre\n",
      "до смерти\n",
      "и как всегда в моих куплетах ктото умрёт\n",
      "мчится вспять цирк и детсад русрэпа моя crew спецназ йе\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    if generate_sent(model,20,random_seed=i) == '':\n",
    "        continue\n",
    "    else:\n",
    "        print(generate_sent(model,20,random_seed=i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4-gram model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, padded_sents = padded_everygram_pipeline(4, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting the model\n",
    "model = MLE(4)\n",
    "model.fit(train_data, padded_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "пулитцер брать\n",
      "ходу рановато выводы сами из неё сделали пацаны\n",
      "но твою ебу под феном и ибупрофеном\n",
      "впереди водоворот\n",
      "захотел по красоте рубить стафф по асфальту мимо цемента\n",
      "оспы\n",
      "знаю девы приведут к проблемам\n",
      "life\n",
      "горестную лыбу бытия ухмылку авгура\n",
      "ты мой дауншифтинг дня я твоя миссия века\n",
      "vgb\n",
      "а ты хоть полумёртвый ты помнишь на том берегу золотое руно\n",
      "четкий посыл\n",
      "лучше с редькиным за руку на буерак\n",
      "в себе щерит волчий оскал рра\n",
      "кольцо проносит и бросит в жерло\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    if generate_sent(model,20,random_seed=i) == '':\n",
    "        continue\n",
    "    else:\n",
    "        print(generate_sent(model,20,random_seed=i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, generated texts are not really good and I highly doubt that we could call them rap :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:\n",
    "1. https://www.kaggle.com/alvations/n-gram-language-model-with-nltk\n",
    "2. https://www.nltk.org/book/ch01.html\n",
    "3. http://www.machinelearning.ru/wiki/images/archive/e/e9/20180912152902%21Mmta18-langmodels.pdf\n",
    "4. https://kite.com/python/docs/nltk.lm\n",
    "5. https://www.geeksforgeeks.org/tokenize-text-using-nltk-python/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
